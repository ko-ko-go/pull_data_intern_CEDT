{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d045ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "cookies = {\n",
    "    'athena_session': 'eyJpdiI6Im1y5utaXHUwMDA0d%2BwgL9neIiwidmFsdWUiOiKiJW69x4SrXHUwMDEyzL3FWFx1MDAxZYe5nWaArFx1MDAwN546eVxcvFx1MDAwZXEpYXbTcUn4XGZcdTAwMDPDMpnPK5xcdTAwMDCTjW%2FQSXR32TzTuyt%2Bjlx1MDAxNuCtscr0Llx1MDAwNVE7mVx1MDAxN4lQW%2BvGQ8V4XHUwMDBlPu42koOA2HHeXHUwMDA2wKNFIymYaO1qlebb7WV53t5cdTAwMDHi2Ub7lqSPWXO83LRPU2SVXHUwMDFlXHUwMDAxXGL%2F3YV5LCFfRoyJyGRjjtlcdTAwMTFcdTAwMTI%2BVVx1MDAxNdNdXHUwMDFl88y%2Fwe1cdTAwMWKSf%2BNcdTAwMTCfXHUwMDFjXHUwMDEzPTG2buH5yPCzrZ16ID8yzcVcdTAwMDVUS9%2BlfoqnallcdTAwMWTjQr1cbkNcdTAwMGXuXHUwMDFhdTtFsrf3ZHvYJmB0ckD4LlLH4ZB6Vc9NLG%2FfJVx1MDAxM1x1MDAwMsUtdoxcdTAwMTC%2FXHUwMDAyWFx1MDAxNWkj%2Blx1MDAxNP9cdTAwMDGPR9BcXFx1MDAwMt%2Bgzlx1MDAxNYpYlXdcIslcdTAwMDAuWDQlWyl6vz1cdTAwMWa1lvK9YmZcdTAwMTOhPJd3OFx1MDAwM%2FVXIEdzuK7NJSNcdCfcWFx1MDAxZODyb1x1MDAxMnwiLCJ0YWciOiLOvk1EoKniXHUwMDA2X8znTXgtRnkifQ%3D%3D',\n",
    "    '_ga_90M1QKG0M0': 'GS2.1.s1769603456$o2$g1$t1769603657$j60$l0$h0',\n",
    "    '_ga': 'GA1.1.450646043.1762079226',\n",
    "    '_ga_QST7JN5ZKL': 'GS2.3.s1767962408$o14$g1$t1767962973$j60$l0$h0',\n",
    "    '_ga_L8ZZDV35P6': 'GS2.1.s1767962408$o10$g1$t1767962959$j60$l0$h0',\n",
    "    '_ga_9E879WD4VX': 'GS2.3.s1767962408$o7$g1$t1767962534$j60$l0$h0',\n",
    "    '_hjSessionUser_2291244': 'eyJpZCI6IjQ4YmVmOGQyLWExNzMtNTI2NC05MDhjLTBjMDEwNjBiN2E5MSIsImNyZWF0ZWQiOjE3NjIyMzg4NDc1MDMsImV4aXN0aW5nIjp0cnVlfQ==',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    # 'Cookie': 'athena_session=eyJpdiI6Im1y5utaXHUwMDA0d%2BwgL9neIiwidmFsdWUiOiKiJW69x4SrXHUwMDEyzL3FWFx1MDAxZYe5nWaArFx1MDAwN546eVxcvFx1MDAwZXEpYXbTcUn4XGZcdTAwMDPDMpnPK5xcdTAwMDCTjW%2FQSXR32TzTuyt%2Bjlx1MDAxNuCtscr0Llx1MDAwNVE7mVx1MDAxN4lQW%2BvGQ8V4XHUwMDBlPu42koOA2HHeXHUwMDA2wKNFIymYaO1qlebb7WV53t5cdTAwMDHi2Ub7lqSPWXO83LRPU2SVXHUwMDFlXHUwMDAxXGL%2F3YV5LCFfRoyJyGRjjtlcdTAwMTFcdTAwMTI%2BVVx1MDAxNdNdXHUwMDFl88y%2Fwe1cdTAwMWKSf%2BNcdTAwMTCfXHUwMDFjXHUwMDEzPTG2buH5yPCzrZ16ID8yzcVcdTAwMDVUS9%2BlfoqnallcdTAwMWTjQr1cbkNcdTAwMGXuXHUwMDFhdTtFsrf3ZHvYJmB0ckD4LlLH4ZB6Vc9NLG%2FfJVx1MDAxM1x1MDAwMsUtdoxcdTAwMTC%2FXHUwMDAyWFx1MDAxNWkj%2Blx1MDAxNP9cdTAwMDGPR9BcXFx1MDAwMt%2Bgzlx1MDAxNYpYlXdcIslcdTAwMDAuWDQlWyl6vz1cdTAwMWa1lvK9YmZcdTAwMTOhPJd3OFx1MDAwM%2FVXIEdzuK7NJSNcdCfcWFx1MDAxZODyb1x1MDAxMnwiLCJ0YWciOiLOvk1EoKniXHUwMDA2X8znTXgtRnkifQ%3D%3D; _ga_90M1QKG0M0=GS2.1.s1769603456$o2$g1$t1769603657$j60$l0$h0; _ga=GA1.1.450646043.1762079226; _ga_QST7JN5ZKL=GS2.3.s1767962408$o14$g1$t1767962973$j60$l0$h0; _ga_L8ZZDV35P6=GS2.1.s1767962408$o10$g1$t1767962959$j60$l0$h0; _ga_9E879WD4VX=GS2.3.s1767962408$o7$g1$t1767962534$j60$l0$h0; _hjSessionUser_2291244=eyJpZCI6IjQ4YmVmOGQyLWExNzMtNTI2NC05MDhjLTBjMDEwNjBiN2E5MSIsImNyZWF0ZWQiOjE3NjIyMzg4NDc1MDMsImV4aXN0aW5nIjp0cnVlfQ==',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.5 Safari/605.1.15',\n",
    "    'Accept-Language': 'th-TH,th;q=0.9',\n",
    "    # 'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Priority': 'u=0, i',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'search': '',\n",
    "    'page': '1',\n",
    "    'onlyBookmarked': 'false',\n",
    "    'onlyAvailablePositions': 'false',\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    'https://cedtintern.cp.eng.chula.ac.th/api/sessions/4/openings',\n",
    "    params=params,\n",
    "    cookies=cookies,\n",
    "    headers=headers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f1de493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API...\n",
      "‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: 100 ‡πÅ‡∏ñ‡∏ß\n",
      "üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Sheets...\n",
      "üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å (Sheet 1)...\n",
      "üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (Sheet 2)...\n",
      "üèÜ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î My Top Picks ‡πÅ‡∏ö‡∏ö Full Data (Sheet 3)...\n",
      "------------------------------\n",
      "‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏£‡∏ö 3 ‡∏´‡∏ô‡πâ‡∏≤\n",
      "   üëâ ‡∏´‡∏ô‡πâ‡∏≤ 3 (My Top Picks) ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & IMPORTS\n",
    "# ==========================================\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import urllib.parse # ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏•‡∏¥‡∏á‡∏Å‡πå\n",
    "\n",
    "# ==========================================\n",
    "# 2. FUNCTIONS\n",
    "# ==========================================\n",
    "def clean_html(text):\n",
    "    if pd.isna(text) or text == \"\" or str(text).lower() == 'none':\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(str(text), \"html.parser\")\n",
    "    clean_text = soup.get_text(separator=' ')\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "def clean_tags(tag_data):\n",
    "    if isinstance(tag_data, list):\n",
    "        if len(tag_data) == 0:\n",
    "            return \"\"\n",
    "        tag_names = [str(t.get('tagName', '')) for t in tag_data if isinstance(t, dict)]\n",
    "        return \", \".join(filter(None, tag_names))\n",
    "    \n",
    "    if pd.isna(tag_data) or tag_data == \"\" or tag_data == \"[]\":\n",
    "        return \"\"\n",
    "    \n",
    "    if isinstance(tag_data, str):\n",
    "        try:\n",
    "            tags_list = ast.literal_eval(tag_data)\n",
    "            if isinstance(tags_list, list):\n",
    "                tag_names = [str(t.get('tagName', '')) for t in tags_list if isinstance(t, dict)]\n",
    "                return \", \".join(filter(None, tag_names))\n",
    "        except:\n",
    "            return str(tag_data)\n",
    "    return str(tag_data)\n",
    "\n",
    "# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏á‡∏≤‡∏ô‡∏™‡∏≤‡∏¢ Data ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ---\n",
    "def check_is_data_role(row):\n",
    "    keywords = [\n",
    "        'data', 'datum', 'sql', 'etl','ETL','ELT','elt', 'pipeline', 'airflow', 'spark', 'hadoop', \n",
    "        'database', 'python', 'pandas', 'visualization', 'dashboard',  \n",
    "        'business intelligence', 'machine learning', 'artificial intelligence',\n",
    "        'big data', 'analyze', 'analytics'\n",
    "    ]\n",
    "    content = f\"{str(row.get('title', ''))} {str(row.get('description', ''))} {str(row.get('requirements', ''))}\".lower()\n",
    "    for word in keywords:\n",
    "        if word in content:\n",
    "            return \"Yes\"\n",
    "    return \"No\"\n",
    "\n",
    "# --- [‡πÉ‡∏´‡∏°‡πà] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Map (‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 5) ---\n",
    "def create_search_link(row):\n",
    "    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å officeAddressLine1 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ç‡∏≠\n",
    "    address = str(row.get('officeAddressLine1', '')).strip()\n",
    "    \n",
    "    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏ß‡πà‡∏≤‡∏á\n",
    "    if not address or address.lower() == 'nan':\n",
    "        return \"\"\n",
    "        \n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô format URL (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ ‡πÄ‡∏õ‡πá‡∏ô %20) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏™‡∏µ‡∏¢\n",
    "    encoded_address = urllib.parse.quote(address)\n",
    "    \n",
    "    # ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "    return f\"https://www.google.com/maps/search/{address}\"\n",
    "\n",
    "# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏≥‡πÄ‡∏• (‡∏â‡∏ö‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î) ---\n",
    "def check_key_locations(row):\n",
    "    # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤\n",
    "    address_text = f\"{str(row.get('officeName', ''))} {str(row.get('officeAddressLine1', ''))} {str(row.get('officeAddressLine2', ''))} {str(row.get('description', ''))}\".lower()\n",
    "    \n",
    "    mrt_key = ['mrt']\n",
    "    for kw in mrt_key:\n",
    "        if kw in address_text:\n",
    "            return \"‚úÖ MRT\"\n",
    "    \n",
    "    bts_in_mrt_key = [\n",
    "        'bts', 'bts khukhot', 'bts ‡∏Ñ‡∏π‡∏Ñ‡∏ï',\n",
    "        'bts yaek kor por aor', 'bts ‡πÅ‡∏¢‡∏Å ‡∏Ñ‡∏õ‡∏≠', 'bts ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏õ‡∏≠',\n",
    "        'bts saphan mai', 'bts ‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà',\n",
    "        'bts sai yud', 'bts ‡∏™‡∏≤‡∏¢‡∏´‡∏¢‡∏∏‡∏î',\n",
    "        'bts wat phra sri', 'bts wat phrasri', 'bts ‡∏ß‡∏±‡∏î‡∏û‡∏£‡∏∞‡∏®‡∏£‡∏µ',\n",
    "        'bts bang bua', 'bts ‡∏ö‡∏≤‡∏á‡∏ö‡∏±‡∏ß',\n",
    "        'bts krom pa mai', 'bts ‡∏Å‡∏£‡∏°‡∏õ‡πà‡∏≤‡πÑ‡∏°‡πâ',\n",
    "        'bts kasetsart', 'bts ku', 'bts ‡∏°.‡πÄ‡∏Å‡∏©‡∏ï‡∏£', 'bts ‡πÄ‡∏Å‡∏©‡∏ï‡∏£',\n",
    "        'bts sena nikhom', 'bts ‡πÄ‡∏™‡∏ô‡∏≤‡∏ô‡∏¥‡∏Ñ‡∏°',\n",
    "        'bts ratchayothin', 'bts ‡∏£‡∏±‡∏ä‡πÇ‡∏¢‡∏ò‡∏¥‡∏ô',\n",
    "        'bts phahon yothin 24', 'bts phahon 24', 'bts ‡∏û‡∏´‡∏•‡πÇ‡∏¢‡∏ò‡∏¥‡∏ô 24', 'bts ‡∏û‡∏´‡∏• 24',\n",
    "        'bts ha yaek lat phrao', 'bts lat phrao intersection', 'bts ‡∏´‡πâ‡∏≤‡πÅ‡∏¢‡∏Å‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß', 'bts 5 ‡πÅ‡∏¢‡∏Å‡∏•‡∏≤‡∏î‡∏û‡∏£‡πâ‡∏≤‡∏ß',\n",
    "        'bts mo chit', 'bts mochit', 'bts morchit', 'bts ‡∏´‡∏°‡∏≠‡∏ä‡∏¥‡∏ï',\n",
    "        'bts saphan khwai', 'bts ‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏¢',\n",
    "        'bts ari', 'bts aree', 'bts ‡∏≠‡∏≤‡∏£‡∏µ‡∏¢‡πå', 'bts ‡∏≠‡∏≤‡∏£‡∏µ',\n",
    "        'bts sanam pao', 'bts ‡∏™‡∏ô‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤',\n",
    "        'bts victory monument', 'bts anusawari', 'bts ‡∏≠‡∏ô‡∏∏‡∏™‡∏≤‡∏ß‡∏£‡∏µ‡∏¢‡πå',\n",
    "        'bts phaya thai', 'bts phayathai', 'bts ‡∏û‡∏ç‡∏≤‡πÑ‡∏ó',\n",
    "        'bts ratchathewi', 'bts ‡∏£‡∏≤‡∏ä‡πÄ‡∏ó‡∏ß‡∏µ',\n",
    "        'bts siam', 'bts sayam', 'bts ‡∏™‡∏¢‡∏≤‡∏°',\n",
    "        'bts chit lom', 'bts chitlom', 'bts chidlom', 'bts ‡∏ä‡∏¥‡∏î‡∏•‡∏°',\n",
    "        'bts phloen chit', 'bts ploenchit', 'bts ‡πÄ‡∏û‡∏•‡∏¥‡∏ô‡∏à‡∏¥‡∏ï',\n",
    "        'bts nana', 'bts ‡∏ô‡∏≤‡∏ô‡∏≤',\n",
    "        'bts asok', 'bts asoke', 'bts ‡∏≠‡πÇ‡∏®‡∏Å',\n",
    "        'bts phrom phong', 'bts promphong', 'bts ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏û‡∏á‡∏©‡πå',\n",
    "        'bts thong lo', 'bts thonglor', 'bts ‡∏ó‡∏≠‡∏á‡∏´‡∏•‡πà‡∏≠',\n",
    "        'bts ekkamai', 'bts ekamai', 'bts ‡πÄ‡∏≠‡∏Å‡∏°‡∏±‡∏¢',\n",
    "        'bts phra khanong', 'bts prakanong', 'bts ‡∏û‡∏£‡∏∞‡πÇ‡∏Ç‡∏ô‡∏á',\n",
    "        'bts on nut', 'bts onnut', 'bts ‡∏≠‡πà‡∏≠‡∏ô‡∏ô‡∏∏‡∏ä',\n",
    "        'bts bang chak', 'bts bangchak', 'bts ‡∏ö‡∏≤‡∏á‡∏à‡∏≤‡∏Å',\n",
    "        'bts punnawithi', 'bts ‡∏õ‡∏∏‡∏ì‡∏ì‡∏ß‡∏¥‡∏ñ‡∏µ',\n",
    "        'bts udom suk', 'bts udomsuk', 'bts ‡∏≠‡∏∏‡∏î‡∏°‡∏™‡∏∏‡∏Ç',\n",
    "        'bts bang na', 'bts bangna', 'bts ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤',\n",
    "        'bts bearing', 'bts ‡πÅ‡∏ö‡∏£‡∏¥‡πà‡∏á',\n",
    "        'bts samrong', 'bts ‡∏™‡∏≥‡πÇ‡∏£‡∏á',\n",
    "        'bts pu chao', 'bts ‡∏õ‡∏π‡πà‡πÄ‡∏à‡πâ‡∏≤',\n",
    "        'bts erawan', 'bts chang erawan', 'bts ‡∏ä‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏£‡∏≤‡∏ß‡∏±‡∏ì',\n",
    "        'bts pak nam', 'bts ‡∏õ‡∏≤‡∏Å‡∏ô‡πâ‡∏≥',\n",
    "        'bts kheha', 'bts ‡πÄ‡∏Ñ‡∏´‡∏∞',\n",
    "        'bts national stadium', 'bts ‡∏™‡∏ô‡∏≤‡∏°‡∏Å‡∏µ‡∏¨‡∏≤',\n",
    "        'bts ratchadamri', 'bts ‡∏£‡∏≤‡∏ä‡∏î‡∏≥‡∏£‡∏¥',\n",
    "        'bts sala daeng', 'bts saladaeng', 'bts ‡∏®‡∏≤‡∏•‡∏≤‡πÅ‡∏î‡∏á',\n",
    "        'bts chong nonsi', 'bts ‡∏ä‡πà‡∏≠‡∏á‡∏ô‡∏ô‡∏ó‡∏£‡∏µ',\n",
    "        'bts saint louis', 'bts st. louis', 'bts ‡πÄ‡∏ã‡∏ô‡∏ï‡πå‡∏´‡∏•‡∏∏‡∏¢‡∏™‡πå',\n",
    "        'bts surasak', 'bts ‡∏™‡∏∏‡∏£‡∏®‡∏±‡∏Å‡∏î‡∏¥‡πå',\n",
    "        'bts saphan taksin', 'bts ‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏ï‡∏≤‡∏Å‡∏™‡∏¥‡∏ô',\n",
    "        'bts krung thon buri', 'bts krung thonburi', 'bts ‡∏Å‡∏£‡∏∏‡∏á‡∏ò‡∏ô‡∏ö‡∏∏‡∏£‡∏µ', 'bts ‡∏Å‡∏£‡∏∏‡∏á‡∏ò‡∏ô',\n",
    "        'bts wongwian yai', 'bts ‡∏ß‡∏á‡πÄ‡∏ß‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡∏ç‡πà',\n",
    "        'bts pho nimit', 'bts ‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏ô‡∏¥‡∏°‡∏¥‡∏ï‡∏£',\n",
    "        'bts talat phlu', 'bts talad phlu', 'bts ‡∏ï‡∏•‡∏≤‡∏î‡∏û‡∏•‡∏π',\n",
    "        'bts wutthakat', 'bts ‡∏ß‡∏∏‡∏í‡∏≤‡∏Å‡∏≤‡∏®',\n",
    "        'bts bang wa', 'bts bangwa', 'bts ‡∏ö‡∏≤‡∏á‡∏´‡∏ß‡πâ‡∏≤',\n",
    "        'bts charoen nakhon', 'bts ‡πÄ‡∏à‡∏£‡∏¥‡∏ç‡∏ô‡∏Ñ‡∏£', 'bts iconsiam', 'bts ‡πÑ‡∏≠‡∏Ñ‡∏≠‡∏ô‡∏™‡∏¢‡∏≤‡∏°',\n",
    "        'bts khlong san', 'bts ‡∏Ñ‡∏•‡∏≠‡∏á‡∏™‡∏≤‡∏ô'\n",
    "    ]\n",
    "\n",
    "    for kw in bts_in_mrt_key:\n",
    "        if kw in address_text:\n",
    "            return \"‚úÖ BTS\" \n",
    "\n",
    "    rama9_keywords = [\n",
    "        'mrt rama 9', 'mrt rama9', 'mrt phra ram 9', 'mrt ‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏° 9', \n",
    "        'central rama 9', '‡πÄ‡∏ã‡πá‡∏ô‡∏ó‡∏£‡∏±‡∏•‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏° 9', 'fortune town', '‡∏ü‡∏≠‡∏£‡πå‡∏à‡∏π‡∏ô',\n",
    "        'g tower', 'g-tower', 'unilever', '‡∏¢‡∏π‡∏ô‡∏¥‡∏•‡∏µ‡πÄ‡∏ß‡∏≠‡∏£‡πå', 'aia capital',\n",
    "        'stock exchange', '‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå', 'true tower', 'belle grand',\n",
    "        'the ninth', 'new cbd', 'jodd fair', '‡∏à‡πä‡∏≠‡∏î‡πÅ‡∏ü‡∏£‡πå'\n",
    "    ]\n",
    "    for kw in rama9_keywords:\n",
    "        if kw in address_text:\n",
    "            return \"‚úÖ Rama 9 / Ratchada\"\n",
    "\n",
    "    victory_mon_keywords = [\n",
    "        'victory monument', 'anusawari', '‡∏≠‡∏ô‡∏∏‡∏™‡∏≤‡∏ß‡∏£‡∏µ‡∏¢‡πå‡∏ä‡∏±‡∏¢', \n",
    "        'phaya thai', '‡∏û‡∏ç‡∏≤‡πÑ‡∏ó', 'ratchathewi', '‡∏£‡∏≤‡∏ä‡πÄ‡∏ó‡∏ß‡∏µ',\n",
    "        'rangnam', '‡∏£‡∏≤‡∏á‡∏ô‡πâ‡∏≥', 'king power', '‡∏Ñ‡∏¥‡∏á‡πÄ‡∏û‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå',\n",
    "        'century', '‡πÄ‡∏ã‡πá‡∏ô‡∏à‡∏π‡∏£‡∏µ‡πà', 'ratchawithi', '‡∏£‡∏≤‡∏ä‡∏ß‡∏¥‡∏ñ‡∏µ',\n",
    "        'yothi', '‡πÇ‡∏¢‡∏ò‡∏µ', 'sanam pao', '‡∏™‡∏ô‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤'\n",
    "    ]\n",
    "    for kw in victory_mon_keywords:\n",
    "        if kw in address_text:\n",
    "            return \"‚úÖ Victory Mon. / Phaya Thai\"\n",
    "\n",
    "    siam_keywords = [\n",
    "        'siam', '‡∏™‡∏¢‡∏≤‡∏°', 'central world', '‡πÄ‡∏ã‡πá‡∏ô‡∏ó‡∏£‡∏±‡∏•‡πÄ‡∏ß‡∏¥‡∏•‡∏î‡πå', \n",
    "        'chula', '‡∏à‡∏∏‡∏¨‡∏≤', 'samyan', '‡∏™‡∏≤‡∏°‡∏¢‡πà‡∏≤‡∏ô', 'mbk', \n",
    "        'pathum wan', '‡∏õ‡∏ó‡∏∏‡∏°‡∏ß‡∏±‡∏ô', 'chamchuri', '‡∏à‡∏≤‡∏°‡∏à‡∏∏‡∏£‡∏µ',\n",
    "        'national stadium', '‡∏™‡∏ô‡∏≤‡∏°‡∏Å‡∏µ‡∏¨‡∏≤‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥'\n",
    "    ]\n",
    "    for kw in siam_keywords:\n",
    "        if kw in address_text:\n",
    "            return \"‚úÖ Siam / Chula\"\n",
    "\n",
    "    rama4_keywords = [\n",
    "        'lumpini', '‡∏•‡∏∏‡∏°‡∏û‡∏¥‡∏ô‡∏µ', 'rama 4', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏° 4', 'rama iv',\n",
    "        'one bangkok', '‡∏ß‡∏±‡∏ô‡πÅ‡∏ö‡∏á‡∏Ñ‡πá‡∏≠‡∏Å', 'qsncc', 'queen sirikit', '‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏™‡∏¥‡∏£‡∏¥‡∏Å‡∏¥‡∏ï‡∏¥‡πå',\n",
    "        'benjakitti', '‡πÄ‡∏ö‡∏ç‡∏à‡∏Å‡∏¥‡∏ï‡∏¥', 'khlong toei', '‡∏Ñ‡∏•‡∏≠‡∏á‡πÄ‡∏ï‡∏¢',\n",
    "        'fyi center', 'the parq', 'medpark', 'ocean tower', 'lake rajada',\n",
    "        'witthayu', 'wireless road', '‡∏ñ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ó‡∏¢‡∏∏', 'sathorn', '‡∏™‡∏≤‡∏ó‡∏£'\n",
    "    ]\n",
    "    for kw in rama4_keywords:\n",
    "        if kw in address_text:\n",
    "            return \"‚úÖ Rama 4 / QSNCC / Sathorn\"\n",
    "\n",
    "    dindaeng_keywords = [\n",
    "        'din daeng', '‡∏î‡∏¥‡∏ô‡πÅ‡∏î‡∏á', 'vibhavadi', '‡∏ß‡∏¥‡∏†‡∏≤‡∏ß‡∏î‡∏µ', \n",
    "        'mit maitri', '‡∏°‡∏¥‡∏ï‡∏£‡πÑ‡∏°‡∏ï‡∏£‡∏µ', 'pracha songkhro', '‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏™‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå',\n",
    "        'utcc', '‡∏´‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤', 'thai-japan', '‡πÑ‡∏ó‡∏¢-‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô'\n",
    "    ]\n",
    "    for kw in dindaeng_keywords:\n",
    "        if kw in address_text:\n",
    "            return \"‚úÖ Din Daeng / Vibhavadi\"\n",
    "\n",
    "    return \"\"\n",
    "# ==========================================\n",
    "# 3. FETCH DATA (Updated with Debugging)\n",
    "# ==========================================\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API...\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if 'items' exists, otherwise look at the keys\n",
    "    if 'items' in data:\n",
    "        df = pd.json_normalize(data['items'])\n",
    "        print(f\"‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df)} ‡πÅ‡∏ñ‡∏ß\")\n",
    "    else:\n",
    "        print(\"‚ùå Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏µ‡∏¢‡πå 'items' ‡πÉ‡∏ô JSON response\")\n",
    "        print(f\"‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏Ñ‡∏∑‡∏≠: {list(data.keys())}\")\n",
    "        print(\"‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö:\")\n",
    "        print(data) # Print this to see what the API actually sent\n",
    "else:\n",
    "    print(f\"‚ùå API Request Failed! Status Code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "# ==========================================\n",
    "# 4. CLEANING & CALCULATION\n",
    "# ==========================================\n",
    "\n",
    "# 4.1 Clean HTML\n",
    "cols_to_clean = ['description', 'requirements']\n",
    "for col in cols_to_clean:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(clean_html)\n",
    "\n",
    "# 4.2 Clean Tags\n",
    "if 'tags' in df.columns:\n",
    "    df['tags'] = df['tags'].apply(clean_tags)\n",
    "\n",
    "# 4.3 Rename Columns\n",
    "new_names = {\n",
    "    'company.companyNameTh': 'company',\n",
    "    'compensationType.compensationType': 'compensation_Type'\n",
    "}\n",
    "df = df.rename(columns=new_names)\n",
    "\n",
    "# 4.4 Convert Types\n",
    "df['compensationAmount'] = pd.to_numeric(df['compensationAmount'], errors='coerce').fillna(0)\n",
    "df['quota'] = pd.to_numeric(df['quota'], errors='coerce').fillna(1)\n",
    "df['inStudentDraftCount'] = pd.to_numeric(df['inStudentDraftCount'], errors='coerce').fillna(0)\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô\n",
    "df['competition_rate'] = df['inStudentDraftCount'] / df['quota']\n",
    "df['competition_rate'] = df['competition_rate'].round(2)\n",
    "df['competition_rate'] = df['competition_rate'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# 4.5 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "df['is_data_job'] = df.apply(check_is_data_role, axis=1)\n",
    "df['location_zone'] = df.apply(check_key_locations, axis=1)\n",
    "\n",
    "# --- [‡πÉ‡∏´‡∏°‡πà] ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ---\n",
    "df['map_search_link'] = df.apply(create_search_link, axis=1)\n",
    "\n",
    "# 4.6 Sort Values\n",
    "df = df.sort_values(by='compensationAmount', ascending=False)\n",
    "\n",
    "# ==========================================\n",
    "# 5. SELECT COLUMNS\n",
    "# ==========================================\n",
    "keep_cols = [\n",
    "    'company', \n",
    "    'title', \n",
    "    'is_data_job',\n",
    "    'location_zone',\n",
    "    'quota', \n",
    "    'inStudentDraftCount',\n",
    "    'competition_rate', \n",
    "    'compensationAmount', \n",
    "    'compensation_Type', \n",
    "    'tags',\n",
    "    'workingCondition', \n",
    "    'officeName',\n",
    "    'officeAddressLine1',\n",
    "    'officeAddressLine2',\n",
    "    'description', \n",
    "    'requirements'\n",
    "]\n",
    "\n",
    "existing_cols = [c for c in keep_cols if c in df.columns]\n",
    "df = df[existing_cols]\n",
    "\n",
    "# ==========================================\n",
    "# [‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏°‡πà] ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (Statistics)\n",
    "# ==========================================\n",
    "# 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°\n",
    "total_jobs = len(df)\n",
    "total_quota = int(df['quota'].sum())\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (>0)\n",
    "avg_salary = df[df['compensationAmount'] > 0]['compensationAmount'].mean()\n",
    "if pd.isna(avg_salary): avg_salary = 0\n",
    "data_jobs_count = len(df[df['is_data_job'] == 'Yes'])\n",
    "\n",
    "# 2. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡πÇ‡∏ã‡∏ô (Location)\n",
    "zone_stats = df['location_zone'].value_counts().reset_index()\n",
    "zone_stats.columns = ['Zone', 'Count']\n",
    "\n",
    "# 3. ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 5 ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏¢‡∏≠‡∏∞‡∏™‡∏∏‡∏î\n",
    "top_companies = df.groupby('company')['quota'].sum().reset_index()\n",
    "top_companies = top_companies.sort_values(by='quota', ascending=False).head(5)\n",
    "\n",
    "# 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á List of Lists ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏•‡∏á Sheet\n",
    "stats_rows = []\n",
    "\n",
    "# -- Header --\n",
    "stats_rows.append([\"üìä DASHBOARD / STATS\", \"\"])\n",
    "stats_rows.append([\"Last Updated\", pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')])\n",
    "stats_rows.append([\"\", \"\"]) \n",
    "\n",
    "# -- Overview --\n",
    "stats_rows.append([\"üìå OVERVIEW\", \"Value\"])\n",
    "stats_rows.append([\"Total Openings (Jobs)\", total_jobs])\n",
    "stats_rows.append([\"Total Quota (Positions)\", total_quota])\n",
    "stats_rows.append([\"Avg Salary (THB)\", round(avg_salary, 2)])\n",
    "stats_rows.append([\"Data/AI Roles Found\", data_jobs_count])\n",
    "stats_rows.append([\"\", \"\"])\n",
    "\n",
    "# -- Zone Breakdown --\n",
    "stats_rows.append([\"üìç LOCATION ZONES\", \"Count\"])\n",
    "for _, row in zone_stats.iterrows():\n",
    "    zone_name = row['Zone'] if row['Zone'] else 'Other/Unknown'\n",
    "    stats_rows.append([zone_name, int(row['Count'])])\n",
    "stats_rows.append([\"\", \"\"])\n",
    "\n",
    "# -- Top Hiring Companies --\n",
    "stats_rows.append([\"üè¢ TOP 5 HIRING COMPANIES\", \"Total Quota\"])\n",
    "for _, row in top_companies.iterrows():\n",
    "    stats_rows.append([row['company'], int(row['quota'])])\n",
    "\n",
    "# ==========================================\n",
    "# 6. EXPORT & UPLOAD (ALL STRING)\n",
    "# ==========================================\n",
    "\n",
    "# 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API (Sheet 1 & 2)\n",
    "df = df.fillna('').astype(str)\n",
    "# ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Match\n",
    "df['title'] = df['title'].str.strip()\n",
    "df['company'] = df['company'].str.strip()\n",
    "\n",
    "df.to_csv('cleaned_data-v3.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (Target List)\n",
    "# ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "my_picks_data = [\n",
    "    (1, \"Data Analyst/Engineer\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πå ‡∏ß‡∏µ‡∏à‡∏µ 3 ‡∏à‡∏≥‡∏Å‡∏±‡∏î (Thairath Online)\"),\n",
    "    (2, \"Recruitment Data Analytic Intern\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏ä‡πâ‡∏≠‡∏õ‡∏õ‡∏µ‡πâ (‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢) ‡∏à‡∏≥‡∏Å‡∏±‡∏î\"),\n",
    "    (3, \"Retail loan credit risk modeling and analytics\", \"‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (4, \"Technology Analyst Intern (IP Managaement Team)\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏õ‡∏π‡∏ô‡∏ã‡∏¥‡πÄ‡∏°‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢ ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (5, \"Facility Systems Automation Intern\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏ä‡πâ‡∏≠‡∏õ‡∏õ‡∏µ‡πâ (‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢) ‡∏à‡∏≥‡∏Å‡∏±‡∏î\"),\n",
    "    (6, \"Data Scientists (Blendata Team)\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏à‡∏µ‡πÄ‡∏≠‡πÄ‡∏ö‡∏¥‡∏• ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (7, \"Data and Analytics Engineer (Blendata Team)\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏à‡∏µ‡πÄ‡∏≠‡πÄ‡∏ö‡∏¥‡∏• ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (8, \"Data Management Intern (SCGP)\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏õ‡∏π‡∏ô‡∏ã‡∏¥‡πÄ‡∏°‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢ ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (9, \"Data Scientist - Structured data/Computer Vision/NLP\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏ã‡∏µ‡∏û‡∏µ‡πÄ‡∏≠‡∏ü ‡πÑ‡∏≠‡∏ó‡∏µ‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏à‡∏≥‡∏Å‡∏±‡∏î (Axons)\"),\n",
    "    (10, \"Data Engineer\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏•‡∏ä‡∏±‡πà‡∏ô ‡∏à‡∏≥‡∏Å‡∏±‡∏î\"),\n",
    "    (11, \"Business Analyst\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡πÑ‡∏ó‡∏°‡πå ‡∏Ñ‡∏≠‡∏ô‡∏ã‡∏±‡∏•‡∏ï‡∏¥‡πâ‡∏á ‡∏à‡∏≥‡∏Å‡∏±‡∏î\"),\n",
    "    (12, \"Business Intelligence\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏•‡∏ä‡∏±‡πà‡∏ô ‡∏à‡∏≥‡∏Å‡∏±‡∏î\"),\n",
    "    (13, \"Quantitative Trainee\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏´‡∏¢‡∏ß‡∏ô‡∏ï‡πâ‡∏≤ (‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢) ‡∏à‡∏≥‡∏Å‡∏±‡∏î\"),\n",
    "    (14, \"Business Analyst\", \"‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (15, \"Data Engineer\", \"‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (16, \"Marketing (Blendata Team)\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏à‡∏µ‡πÄ‡∏≠‡πÄ‡∏ö‡∏¥‡∏• ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (17, \"Business Development (Blendata Team)\", \"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏à‡∏µ‡πÄ‡∏≠‡πÄ‡∏ö‡∏¥‡∏• ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\"),\n",
    "    (18, \"Credit Risk Analytics\", \"‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå ‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)\")\n",
    "]\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
    "df_picks = pd.DataFrame(my_picks_data, columns=['My_Rank', 'title', 'company'])\n",
    "df_picks['title'] = df_picks['title'].str.strip()\n",
    "df_picks['company'] = df_picks['company'].str.strip()\n",
    "\n",
    "# 3. [‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç] ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å DB (df) ‡∏°‡∏≤‡πÉ‡∏™‡πà‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å\n",
    "# ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£ Merge ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á Salary, Link, Description ‡∏Ø‡∏•‡∏Ø ‡∏°‡∏≤‡πÅ‡∏õ‡∏∞‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á\n",
    "df_picks_full = pd.merge(df_picks, df, on=['title', 'company'], how='left')\n",
    "\n",
    "# ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ My Top Picks ‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°\n",
    "show_cols = [\n",
    "    'My_Rank', 'company', \n",
    "    'title', \n",
    "    'is_data_job',\n",
    "    'location_zone',\n",
    "    'quota', \n",
    "    'inStudentDraftCount',\n",
    "    'competition_rate', \n",
    "    'compensationAmount', \n",
    "    'compensation_Type', \n",
    "    'tags',\n",
    "    'workingCondition', \n",
    "    'officeName',\n",
    "    'officeAddressLine1',\n",
    "    'officeAddressLine2',\n",
    "    'description', \n",
    "    'requirements'\n",
    "]\n",
    "# ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error)\n",
    "final_cols = [c for c in show_cols if c in df_picks_full.columns]\n",
    "df_picks_full = df_picks_full[final_cols].fillna('Not Found / Closed') # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ß‡πà‡∏≤ Not Found\n",
    "\n",
    "try:\n",
    "    print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Google Sheets...\")\n",
    "    \n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('key.json', scope) \n",
    "    client = gspread.authorize(creds)\n",
    "    spreadsheet = client.open(\"intern-cho\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # PART A: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å (Sheet 1)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å (Sheet 1)...\")\n",
    "    sheet1 = spreadsheet.sheet1\n",
    "    sheet1.clear()\n",
    "    sheet1.update(values=[df.columns.values.tolist()] + df.values.tolist())\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # PART B: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (Sheet 2)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (Sheet 2)...\")\n",
    "    try:\n",
    "        sheet2 = spreadsheet.get_worksheet(1)\n",
    "        if sheet2 is None: raise Exception\n",
    "    except:\n",
    "        sheet2 = spreadsheet.add_worksheet(title=\"Statistics\", rows=\"100\", cols=\"10\")\n",
    "\n",
    "    sheet2.update_title(\"Statistics\")\n",
    "    sheet2.clear()\n",
    "    sheet2.update(values=stats_rows)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # PART C: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ñ‡∏£‡∏ö (Sheet 3)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"üèÜ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î My Top Picks ‡πÅ‡∏ö‡∏ö Full Data (Sheet 3)...\")\n",
    "    try:\n",
    "        sheet3 = spreadsheet.get_worksheet(2)\n",
    "        if sheet3 is None: raise Exception\n",
    "    except:\n",
    "        sheet3 = spreadsheet.add_worksheet(title=\"My Top Picks\", rows=\"50\", cols=\"15\")\n",
    "\n",
    "    sheet3.update_title(\"My Top Picks\")\n",
    "    \n",
    "    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î\n",
    "    pick_data_upload = [df_picks_full.columns.values.tolist()] + df_picks_full.astype(str).values.tolist()\n",
    "    \n",
    "    sheet3.clear()\n",
    "    sheet3.update(values=pick_data_upload)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏£‡∏ö 3 ‡∏´‡∏ô‡πâ‡∏≤\")\n",
    "    print(\"   üëâ ‡∏´‡∏ô‡πâ‡∏≤ 3 (My Top Picks) ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
